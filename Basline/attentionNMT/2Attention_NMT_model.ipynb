{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_NMT(nn.Module):\n",
    "    def __init__(self,source_vocab_size,target_vocab_size,embedding_size,\n",
    "                 source_length,target_length,lstm_size,batch_size = 32):\n",
    "        super(Attention_NMT,self).__init__()\n",
    "        self.source_embedding =nn.Embedding(source_vocab_size,embedding_size) # source_vocab_size * embedding_size\n",
    "        self.target_embedding = nn.Embedding(target_vocab_size,embedding_size) # target_vocab_size * embedding_size\n",
    "        self.encoder = nn.LSTM(input_size=embedding_size,hidden_size=lstm_size,num_layers=1,\n",
    "                               bidirectional=True,batch_first=True) # if batch_first==False: input_shape=[length,batch_size,embedding_size]\n",
    "        self.decoder = nn.LSTM(input_size=embedding_size+2*lstm_size,hidden_size=lstm_size,num_layers=1,\n",
    "                               batch_first=True)\n",
    "        self.attention_fc_1 = nn.Linear(3*lstm_size, 3*lstm_size) # 注意力机制全连接层1\n",
    "        self.attention_fc_2 = nn.Linear(3 * lstm_size, 1) # 注意力机制全连接层2\n",
    "        self.class_fc_1 = nn.Linear(embedding_size+2*lstm_size+lstm_size, 2*lstm_size) # 分类全连接层1\n",
    "        self.class_fc_2 = nn.Linear(2*lstm_size, target_vocab_size) # 分类全连接层2\n",
    "        \n",
    "    def attention_forward(self,input_embedding,dec_prev_hidden,enc_output):\n",
    "        prev_dec_h = dec_prev_hidden[0].squeeze().unsqueeze(1).repeat(1, 100, 1) # batch_size*legnth*lstm_size\n",
    "        atten_input = torch.cat([enc_output, prev_dec_h], dim=-1) # batch_size*lentth*(3*lstm_size)\n",
    "        attention_weights = self.attention_fc_2(F.relu(self.attention_fc_1(atten_input))) # batch_size*length*1\n",
    "        attention_weights = F.softmax(attention_weights, dim=1) # alpha: batch_size*length*1\n",
    "        atten_output = torch.sum(attention_weights * enc_output, dim=1).unsqueeze(1) # bs*1*(2*lstm_size)\n",
    "        dec_lstm_input = torch.cat([input_embedding, atten_output], dim=2) # bs*1*(embedding_size*2*lstm_size)\n",
    "        dec_output, dec_hidden = self.decoder(dec_lstm_input, dec_prev_hidden)\n",
    "        # dec_output: bs*1*lstm_size\n",
    "        # dec_hidden: [bs*1*lstm_size,bs*1*lstm_size]\n",
    "        return atten_output,dec_output,dec_hidden\n",
    "    \n",
    "    def forward(self, source_data,target_data, mode = \"train\",is_gpu=True):\n",
    "        source_data_embedding = self.source_embedding(source_data) # batch_size*length*embedding_size\n",
    "        enc_output, enc_hidden = self.encoder(source_data_embedding)\n",
    "        # enc_output.shape: batch_size*length*(2*lstm_size) 只返回所有hidden, concat\n",
    "        # enc_hidden：[[h1,h2],[c1,c2]] 返回每个方向最后一个时间步的h和c\n",
    "        self.atten_outputs = Variable(torch.zeros(target_data.shape[0],\n",
    "                                                  target_data.shape[1],\n",
    "                                                  enc_output.shape[2])) # batch_size*length*(2*lstm_size)\n",
    "        self.dec_outputs = Variable(torch.zeros(target_data.shape[0],\n",
    "                                                target_data.shape[1],\n",
    "                                                enc_hidden[0].shape[2])) # batch_size*length*lstm_size\n",
    "        if is_gpu:\n",
    "            self.atten_outputs = self.atten_outputs.cuda()\n",
    "            self.dec_outputs = self.dec_outputs.cuda()\n",
    "        # enc_output: bs*length*(2*lstm_size)\n",
    "        if mode==\"train\": \n",
    "            target_data_embedding = self.target_embedding(target_data) # batch_size*length*embedding_size\n",
    "            dec_prev_hidden = [enc_hidden[0][0].unsqueeze(0),enc_hidden[1][0].unsqueeze(0)]\n",
    "            \n",
    "            # dec_prev_hidden[0]: 1*bs*lstm_size, dec_prev_hidden[1]: 1*bs*lstm_size\n",
    "\n",
    "            for i in range(100):\n",
    "                input_embedding = target_data_embedding[:,i,:].unsqueeze(1) # bs*1*embedding_size\n",
    "                atten_output, dec_output, dec_hidden = self.attention_forward(input_embedding,\n",
    "                                                                              dec_prev_hidden,\n",
    "                                                                              enc_output)\n",
    "                self.atten_outputs[:,i] = atten_output.squeeze()\n",
    "                self.dec_outputs[:,i] = dec_output.squeeze()\n",
    "                dec_prev_hidden = dec_hidden\n",
    "            class_input = torch.cat([target_data_embedding,self.atten_outputs,self.dec_outputs],dim=2) # bs*length*(embedding_size*2*lstm_size+lstm_size)\n",
    "            outs = self.class_fc_2(F.relu(self.class_fc_1(class_input)))\n",
    "        else:\n",
    "            input_embedding = self.target_embedding(target_data)\n",
    "            dec_prev_hidden = [enc_hidden[0][0].unsqueeze(0),enc_hidden[1][0].unsqueeze(0)]\n",
    "            outs = []\n",
    "            for i in range(100):\n",
    "                atten_output, dec_output, dec_hidden = self.attention_forward(input_embedding,\n",
    "                                                                              dec_prev_hidden,\n",
    "                                                                              enc_output)\n",
    "\n",
    "                class_input = torch.cat([input_embedding,atten_output,dec_output],dim=2)\n",
    "                pred = self.class_fc_2(F.relu(self.class_fc_1(class_input)))\n",
    "                pred = torch.argmax(pred,dim=-1)\n",
    "                outs.append(pred.squeeze().cpu().numpy())\n",
    "                dec_prev_hidden = dec_hidden\n",
    "                input_embedding = self.target_embedding(pred)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100, 30000])\n",
      "(100, 64)\n"
     ]
    }
   ],
   "source": [
    "deep_nmt = Attention_NMT(source_vocab_size=30000,target_vocab_size=30000,embedding_size=256,\n",
    "                 source_length=100,target_length=100,lstm_size=256, batch_size=32)\n",
    "source_data = torch.Tensor(np.zeros([64,100])).long()\n",
    "target_data = torch.Tensor(np.zeros([64,100])).long()\n",
    "preds = deep_nmt(source_data,target_data,is_gpu=False)\n",
    "print (preds.shape)\n",
    "target_data = torch.Tensor(np.zeros([64, 1])).long()\n",
    "preds = deep_nmt(source_data, target_data,mode=\"test\",is_gpu=False)\n",
    "print(np.array(preds).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
