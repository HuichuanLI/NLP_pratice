{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10961b630>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from model import HAN_Model\n",
    "from data import IMDB_Data\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import config as argumentparser\n",
    "config = argumentparser.ArgumentParser()\n",
    "torch.manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as argumentparser\n",
    "config = argumentparser.ArgumentParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.cuda and torch.cuda.is_available():  # 是否使用gpu\n",
    "    torch.cuda.set_device(config.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() # 查看gpu是否可用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 280593\n",
      "10000 280593\n",
      "20000 280593\n",
      "30000 280593\n",
      "40000 280593\n",
      "50000 280593\n",
      "60000 280593\n",
      "70000 280593\n",
      "80000 280593\n",
      "90000 280593\n",
      "100000 280593\n",
      "110000 280593\n",
      "120000 280593\n",
      "130000 280593\n",
      "140000 280593\n",
      "150000 280593\n",
      "160000 280593\n",
      "170000 280593\n",
      "180000 280593\n",
      "190000 280593\n",
      "200000 280593\n",
      "210000 280593\n",
      "220000 280593\n",
      "230000 280593\n",
      "240000 280593\n",
      "250000 280593\n",
      "260000 280593\n",
      "270000 280593\n",
      "280000 280593\n"
     ]
    }
   ],
   "source": [
    "# 导入训练集\n",
    "training_set = IMDB_Data(\"imdb-train.txt.ss\",min_count=config.min_count,\n",
    "                         max_sentence_length = config.max_sentence_length,batch_size=config.batch_size,is_pretrain=False)\n",
    "training_iter = torch.utils.data.DataLoader(dataset=training_set,\n",
    "                                            batch_size=config.batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 34029\n",
      "10000 34029\n",
      "20000 34029\n",
      "30000 34029\n"
     ]
    }
   ],
   "source": [
    "# 导入测试集\n",
    "test_set = IMDB_Data(\"imdb-test.txt.ss\",min_count=config.min_count,word2id=training_set.word2id,\n",
    "                         max_sentence_length = config.max_sentence_length,batch_size=config.batch_size)\n",
    "test_iter = torch.utils.data.DataLoader(dataset=test_set,\n",
    "                                        batch_size=config.batch_size,\n",
    "                                        shuffle=False,\n",
    "                                        num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HAN_Model(vocab_size=len(training_set.word2id),\n",
    "                  embedding_size=config.embedding_size,\n",
    "                  gru_size = config.gru_size,class_num=config.class_num,weights=training_set.weight,is_pretrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.cuda and torch.cuda.is_available(): # 如果使用gpu，将模型送进gpu\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # 这里会做softmax\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "loss = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_result(data_iter,data_set):\n",
    "    # 生成测试结果\n",
    "    model.eval()\n",
    "    true_sample_num = 0\n",
    "    for data, label in data_iter:\n",
    "        if config.cuda and torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            label = label.cuda()\n",
    "        else:\n",
    "            data = torch.autograd.Variable(data).long()\n",
    "        if config.cuda and torch.cuda.is_available():\n",
    "            out = model(data, gpu=True)\n",
    "        else:\n",
    "            out = model(data)\n",
    "        true_sample_num += np.sum((torch.argmax(out, 1) == label).cpu().numpy())\n",
    "    acc = true_sample_num / data_set.__len__()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 627/4385 [11:11<1:07:02,  1.07s/it, loss=1.45]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m     process_bar\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mloss_now\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m get_test_result(test_iter, test_set)\n",
      "File \u001b[0;32m~/miniconda/envs/tf_test/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/tf_test/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(config.epoch):\n",
    "    model.train()\n",
    "    process_bar = tqdm(training_iter)\n",
    "    for data, label in process_bar:\n",
    "        if config.cuda and torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            label = label.cuda()\n",
    "        else:\n",
    "            data = torch.autograd.Variable(data).long()\n",
    "        label = torch.autograd.Variable(label).squeeze()\n",
    "        if config.cuda and torch.cuda.is_available():\n",
    "            out = model(data,gpu=True)\n",
    "        else:\n",
    "            out = model(data)\n",
    "        loss_now = criterion(out, autograd.Variable(label.long()))\n",
    "        if loss == -1:\n",
    "            loss = loss_now.data.item()\n",
    "        else:\n",
    "            loss = 0.95*loss+0.05*loss_now.data.item()\n",
    "        process_bar.set_postfix(loss=loss_now.data.item())\n",
    "        process_bar.update()\n",
    "        optimizer.zero_grad()\n",
    "        loss_now.backward()\n",
    "        optimizer.step()\n",
    "    test_acc = get_test_result(test_iter, test_set)\n",
    "    print(\"The test acc is: %.5f\" % test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
