{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dot(H_e, h_d, fc_layer_1=None, fc_layer_2=None, fc_layer_3=None):\n",
    "    '''\n",
    "    Get attention score throught dot function\n",
    "    :param H_e: encoder hiddens as batch_size * source_length * hidden_size\n",
    "    :param h_d:decoder hidden as batch_size * hidden_size\n",
    "    :return: attention score as batch_size * length * 1\n",
    "    '''\n",
    "    h_d = h_d.unsqueeze(2)\n",
    "    #bs*len*hidden_size . bs * hidden_size*1\n",
    "    attention_score = torch.matmul(H_e, h_d) # batch_size * source_length * 1\n",
    "    attention_score = F.softmax(attention_score, dim=1)\n",
    "    return attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_generate(H_e, h_d,fc_layer_1=None, fc_layer_2=None, fc_layer_3=None):\n",
    "    H_e = fc_layer_1(H_e)\n",
    "    attention_score = score_dot(H_e, h_d)\n",
    "    return attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_concat(H_e, h_d, fc_layer_1=None, fc_layer_2=None, fc_layer_3=None):\n",
    "    h_d = h_d.unsqueeze(1).repeat([1, H_e.size()[1], 1])\n",
    "    # bs*hidden_size -> bs*1*hidden_size -> bs*len*hidden_size\n",
    "    attention_score = fc_layer_3(F.tanh(fc_layer_2(torch.cat([H_e, h_d], dim=2)))) # bs*len*2hidden_size -> bs*len*hidden_size -> bs *len*1\n",
    "    attention_score = F.softmax(attention_score, dim=1)\n",
    "    return attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_m(h_d, t):\n",
    "    pt = torch.ones([h_d.size()[0],1]) * t\n",
    "    return pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_p(h_d, t, fc_layer_1, fc_layer_2, seq_len):\n",
    "    pt = seq_len * F.sigmoid(fc_layer_2(F.tanh(fc_layer_1(h_d))))\n",
    "    return pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_score(attention_score, pt, seq_len, sigma):\n",
    "    pt = pt.unsqueeze(2)\n",
    "    s = torch.range(0, seq_len-1)\n",
    "    s = s.view([1, seq_len, 1]).repeat([attention_score.size()[0],1,1])\n",
    "    attention_score = attention_score * torch.exp(-(s - pt)**2/(2*sigma**2))\n",
    "    attention_score = attention_score/torch.sum(attention_score, dim=1, keepdim=True)\n",
    "    return attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loung_NMT(nn.Module):\n",
    "    def __init__(self,source_vocab_size,target_vocab_size,embedding_size,\n",
    "                 lstm_size, score_f, attention_c, feed_input, window_size=10, reverse=True):\n",
    "        super(Loung_NMT,self).__init__()\n",
    "        self.score_f = score_f\n",
    "        self.attention_c = attention_c\n",
    "        self.window_size = window_size\n",
    "        self.feed_input = feed_input\n",
    "        self.reverse = reverse\n",
    "        self.source_embedding =nn.Embedding(source_vocab_size,embedding_size)\n",
    "        self.target_embedding = nn.Embedding(target_vocab_size,embedding_size)\n",
    "        self.encoder = nn.LSTM(input_size=embedding_size,hidden_size=lstm_size,num_layers=1,\n",
    "                               batch_first=True) # seq_len*batch_size*embedding_size , batch_size*seq_len*embedding_size\n",
    "        if not feed_input:\n",
    "            self.decoder = nn.LSTM(input_size=embedding_size, hidden_size=lstm_size, num_layers=1,\n",
    "                                   batch_first=True)\n",
    "        else:\n",
    "            self.decoder = nn.LSTM(input_size=embedding_size+lstm_size,hidden_size=lstm_size,num_layers=1,\n",
    "                                   batch_first=True)\n",
    "        self.class_fc_1 = nn.Linear(lstm_size+lstm_size, lstm_size) # 分类全连接层1\n",
    "        self.class_fc_2 = nn.Linear(lstm_size, target_vocab_size) # 分类全连接层2\n",
    "\n",
    "        self.attention_fc_1 = nn.Linear(lstm_size, lstm_size)\n",
    "        self.attention_fc_2 = nn.Linear(2*lstm_size, lstm_size)\n",
    "        self.attention_fc_3 = nn.Linear(lstm_size, 1)\n",
    "\n",
    "        self.local_fc_1 = nn.Linear(lstm_size, lstm_size)\n",
    "        self.local_fc_2 = nn.Linear(lstm_size, 1)\n",
    "\n",
    "    def attention_forward(self,input_embedding, feed_input_h, dec_prev_hidden, enc_output, t):\n",
    "        if not self.feed_input:\n",
    "            dec_lstm_input = input_embedding\n",
    "        else:\n",
    "            dec_lstm_input = torch.cat([input_embedding, feed_input_h], dim=2) # bs * 1 * (embed_size+hidden_size)\n",
    "        dec_output, dec_hidden  = self.decoder(dec_lstm_input, dec_prev_hidden) \n",
    "        # dec_output: bs*1*lstm_size, dec_hidden:(1*bs*lstm_size, 1*bs*lstm_size)\n",
    "        if self.score_f == \"dot\":\n",
    "            attention_weights = score_dot(enc_output, dec_hidden[0].squeeze(), self.attention_fc_1, self.attention_fc_2, self.attention_fc_3)\n",
    "        elif self.score_f == \"general\":\n",
    "            attention_weights = score_generate(enc_output, dec_hidden[0].squeeze(), self.attention_fc_1, self.attention_fc_2, self.attention_fc_3)\n",
    "        elif self.score_f == \"concat\":\n",
    "            attention_weights = score_concat(enc_output, dec_hidden[0].squeeze(), self.attention_fc_1, self.attention_fc_2, self.attention_fc_3)\n",
    "        else:\n",
    "            print (\"Attention score function input error!\")\n",
    "            exit()\n",
    "        if self.attention_c == \"local_m\":\n",
    "            if self.reverse:\n",
    "                t = enc_output.size()[1]-1-t\n",
    "            pt = local_m(dec_hidden[0].squeeze(), t)\n",
    "            attention_weights = local_score(attention_weights, pt, enc_output.size()[1], self.window_size/2)\n",
    "        elif self.attention_c == \"local_p\":\n",
    "            pt = local_p(dec_hidden[0].squeeze(), t, self.local_fc_1, self.local_fc_2, enc_output.size()[1])\n",
    "            attention_weights = local_score(attention_weights, pt, enc_output.size()[1], self.window_size / 2)\n",
    "        elif self.attention_c == \"global\":\n",
    "            pass\n",
    "        else:\n",
    "            print (\"Attention class input error!\")\n",
    "            exit()\n",
    "        atten_output = torch.sum(attention_weights * enc_output, dim=1).unsqueeze(1) # bs*1*hidden_size\n",
    "        return atten_output,dec_output,dec_hidden\n",
    "    def forward(self, source_data,target_data, mode = \"train\",is_gpu=True):\n",
    "        source_data_embedding = self.source_embedding(source_data)\n",
    "        enc_output, enc_hidden = self.encoder(source_data_embedding)\n",
    "        # enc_output: bs*len*hidden_size, (1*bs*hidden_size, 1*bs*hidden_size)\n",
    "        self.atten_outputs = Variable(torch.zeros(target_data.shape[0],\n",
    "                                                  target_data.shape[1],\n",
    "                                                  enc_output.shape[2]))\n",
    "        self.dec_outputs = Variable(torch.zeros(target_data.shape[0],\n",
    "                                                target_data.shape[1],\n",
    "                                                enc_hidden[0].shape[2]))\n",
    "        if is_gpu:\n",
    "            self.atten_outputs = self.atten_outputs\n",
    "            self.dec_outputs = self.dec_outputs\n",
    "        # enc_output: bs*length*(2*lstm_size)\n",
    "        if mode==\"train\":\n",
    "            target_data_embedding = self.target_embedding(target_data)\n",
    "            dec_prev_hidden = [enc_hidden[0],enc_hidden[1]]\n",
    "            # dec_prev_hidden[0]: 1*bs*lstm_size, dec_prev_hidden[1]: 1*bs*lstm_size\n",
    "            # dec_h: bs*lstm_size\n",
    "            feed_input_h = enc_hidden[0].squeeze().unsqueeze(1) # 1*bs*hidden_size -> bs*hidden_size -> bs *1 *hidden_size\n",
    "            for i in range(100):\n",
    "                input_embedding = target_data_embedding[:,i,:].unsqueeze(1)  # bs *1 *embedding_size\n",
    "                atten_output, dec_output, dec_hidden = self.attention_forward(input_embedding,\n",
    "                                                                              feed_input_h,\n",
    "                                                                              dec_prev_hidden,\n",
    "                                                                              enc_output, i)\n",
    "                self.atten_outputs[:,i] = atten_output.squeeze()\n",
    "                self.dec_outputs[:,i] = dec_output.squeeze()\n",
    "                dec_prev_hidden = dec_hidden\n",
    "                feed_input_h = F.tanh(self.class_fc_1(torch.cat([atten_output,dec_output],dim=2)))\n",
    "            outs = self.class_fc_2(F.tanh(self.class_fc_1(torch.cat([self.atten_outputs,self.dec_outputs],dim=2))))\n",
    "        else:\n",
    "            input_embedding = self.target_embedding(target_data)\n",
    "            dec_prev_hidden = [enc_hidden[0], enc_hidden[1]]\n",
    "            outs = []\n",
    "            feed_input_h = enc_hidden[0].squeeze(0).unsqueeze(1)\n",
    "            for i in range(100):\n",
    "                atten_output, dec_output, dec_hidden = self.attention_forward(input_embedding,\n",
    "                                                                              feed_input_h,\n",
    "                                                                              dec_prev_hidden,\n",
    "                                                                              enc_output, i)\n",
    "\n",
    "                feed_input_h = F.tanh(self.class_fc_1(torch.cat([atten_output,dec_output],dim=2)))\n",
    "                pred = self.class_fc_2(feed_input_h)\n",
    "                pred = torch.argmax(pred,dim=-1)\n",
    "                outs.append(pred.squeeze().cpu().numpy())\n",
    "                dec_prev_hidden = dec_hidden\n",
    "                input_embedding = self.target_embedding(pred)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100, 30000])\n",
      "(100, 64)\n"
     ]
    }
   ],
   "source": [
    "deep_nmt = Loung_NMT(source_vocab_size=30000,target_vocab_size=30000,embedding_size=256,\n",
    "                         lstm_size=256, score_f=\"dot\", attention_c=\"global\", feed_input=True, window_size=10, reverse=True)\n",
    "source_data = torch.Tensor(np.zeros([64,100])).long()\n",
    "target_data = torch.Tensor(np.zeros([64,100])).long()\n",
    "preds = deep_nmt(source_data,target_data,is_gpu=False)\n",
    "print (preds.shape)\n",
    "target_data = torch.Tensor(np.zeros([64, 1])).long()\n",
    "preds = deep_nmt(source_data, target_data,mode=\"test\",is_gpu=False)\n",
    "print(np.array(preds).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
